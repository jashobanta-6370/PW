{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dimgAqQbboi1"
      },
      "outputs": [],
      "source": [
        "# 1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "A Decision Tree is a supervised machine learning model that splits data into branches based on feature values to make decisions.\n",
        "In classification, it recursively partitions the dataset into subsets using rules based on feature values until it reaches a decision (leaf node).\n",
        "Each internal node represents a decision rule, branches represent outcomes, and leaves represent class labels.\n",
        "\n",
        "# 2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "- **Gini Impurity**: Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "  Formula: Gini = 1 - Σ(pᵢ²) where pᵢ is the probability of class i.\n",
        "- **Entropy**: Measures the disorder or impurity in a set.\n",
        "  Formula: Entropy = -Σ(pᵢ * log₂(pᵢ))\n",
        "Impact: The algorithm selects splits that minimize impurity (maximizing Information Gain).\n",
        "\n",
        "# 3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "- Pre-Pruning: Stops tree growth early by setting constraints like max_depth or min_samples_split.\n",
        "  Advantage: Reduces overfitting and saves computation time.\n",
        "- Post-Pruning: Grows a full tree first, then removes branches that do not improve accuracy.\n",
        "  Advantage: Often results in better generalization.\n",
        "\n",
        "# 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "Information Gain measures the reduction in impurity after a dataset is split.\n",
        "Formula: IG = Impurity(parent) - [weighted average of Impurity(children)]\n",
        "Importance: Higher IG means the split creates more homogeneous child nodes, improving prediction accuracy.\n",
        "\n",
        "# 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Applications:\n",
        "- Medical diagnosis\n",
        "- Credit risk scoring\n",
        "- Fraud detection\n",
        "- Customer segmentation\n",
        "Advantages:\n",
        "- Easy to interpret and visualize\n",
        "- Handles both numerical and categorical data\n",
        "Limitations:\n",
        "- Prone to overfitting\n",
        "- Can be unstable with small changes in data\n",
        "\n",
        "# 6. Python program: Load Iris Dataset, train Decision Tree Classifier using Gini criterion, print accuracy and feature importances.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini')\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy:\", clf_gini.score(X_test, y_test))\n",
        "print(\"Feature Importances:\", clf_gini.feature_importances_)\n",
        "\n",
        "# 7. Python program: Train Decision Tree Classifier with max_depth=3 and compare accuracy to fully-grown tree.\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy (max_depth=3):\", clf_depth3.score(X_test, y_test))\n",
        "print(\"Accuracy (full tree):\", clf_full.score(X_test, y_test))\n",
        "\n",
        "# 8. Python program: Load California Housing dataset, train Decision Tree Regressor, print MSE and feature importances.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train_h, y_train_h)\n",
        "y_pred_h = reg.predict(X_test_h)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test_h, y_pred_h))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n",
        "\n",
        "# 9. Python program: Tune Decision Tree max_depth and min_samples_split using GridSearchCV, print best parameters and accuracy.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'max_depth': [2, 3, 4, 5, None], 'min_samples_split': [2, 5, 10]}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n",
        "# 10. Imagine you’re working as a data scientist for a healthcare company...\n",
        "Step-by-step approach:\n",
        "1) Handle Missing Values:\n",
        "   - For numerical: Fill with median.\n",
        "   - For categorical: Fill with mode.\n",
        "2) Encode Categorical Features:\n",
        "   - Use one-hot encoding for nominal variables.\n",
        "3) Train a Decision Tree Model:\n",
        "   - Use DecisionTreeClassifier with initial parameters.\n",
        "4) Hyperparameter Tuning:\n",
        "   - Use GridSearchCV to tune max_depth, min_samples_split, and criterion.\n",
        "5) Evaluate Performance:\n",
        "   - Use Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "Business Value:\n",
        "- The model can quickly identify at-risk patients.\n",
        "- Enables targeted treatment, reduces healthcare costs, and improves patient outcomes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSgIgUhmbwnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "A Decision Tree is a supervised machine learning model that splits data into branches based on feature values to make decisions.  \n",
        "In classification, it recursively partitions the dataset into subsets using rules based on feature values until it reaches a decision (leaf node).  \n",
        "Each internal node represents a decision rule, branches represent outcomes, and leaves represent class labels.\n",
        "\n",
        "# 2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "- **Gini Impurity**: Measures the probability of incorrectly classifying a randomly chosen element.  \n",
        "  Formula: Gini = 1 - Σ(pᵢ²) where pᵢ is the probability of class i.  \n",
        "- **Entropy**: Measures the disorder or impurity in a set.  \n",
        "  Formula: Entropy = -Σ(pᵢ * log₂(pᵢ))  \n",
        "Impact: The algorithm selects splits that minimize impurity (maximizing Information Gain).\n",
        "\n",
        "# 3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "- Pre-Pruning: Stops tree growth early by setting constraints like max_depth or min_samples_split.  \n",
        "  Advantage: Reduces overfitting and saves computation time.  \n",
        "- Post-Pruning: Grows a full tree first, then removes branches that do not improve accuracy.  \n",
        "  Advantage: Often results in better generalization.\n",
        "\n",
        "# 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "Information Gain measures the reduction in impurity after a dataset is split.  \n",
        "Formula: IG = Impurity(parent) - [weighted average of Impurity(children)]  \n",
        "Importance: Higher IG means the split creates more homogeneous child nodes, improving prediction accuracy.\n",
        "\n",
        "# 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Applications:  \n",
        "- Medical diagnosis  \n",
        "- Credit risk scoring  \n",
        "- Fraud detection  \n",
        "- Customer segmentation  \n",
        "Advantages:  \n",
        "- Easy to interpret and visualize  \n",
        "- Handles both numerical and categorical data  \n",
        "Limitations:  \n",
        "- Prone to overfitting  \n",
        "- Can be unstable with small changes in data\n",
        "\n",
        "# 6. Python program: Load Iris Dataset, train Decision Tree Classifier using Gini criterion, print accuracy and feature importances.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini')\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy:\", clf_gini.score(X_test, y_test))\n",
        "print(\"Feature Importances:\", clf_gini.feature_importances_)\n",
        "\n",
        "# 7. Python program: Train Decision Tree Classifier with max_depth=3 and compare accuracy to fully-grown tree.\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy (max_depth=3):\", clf_depth3.score(X_test, y_test))\n",
        "print(\"Accuracy (full tree):\", clf_full.score(X_test, y_test))\n",
        "\n",
        "# 8. Python program: Load California Housing dataset, train Decision Tree Regressor, print MSE and feature importances.\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train_h, y_train_h)\n",
        "y_pred_h = reg.predict(X_test_h)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test_h, y_pred_h))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n",
        "\n",
        "# 9. Python program: Tune Decision Tree max_depth and min_samples_split using GridSearchCV, print best parameters and accuracy.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'max_depth': [2, 3, 4, 5, None], 'min_samples_split': [2, 5, 10]}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n",
        "# 10. Imagine you’re working as a data scientist for a healthcare company...\n",
        "Step-by-step approach:\n",
        "1) Handle Missing Values:\n",
        "   - For numerical: Fill with median.\n",
        "   - For categorical: Fill with mode.\n",
        "2) Encode Categorical Features:\n",
        "   - Use one-hot encoding for nominal variables.\n",
        "3) Train a Decision Tree Model:\n",
        "   - Use DecisionTreeClassifier with initial parameters.\n",
        "4) Hyperparameter Tuning:\n",
        "   - Use GridSearchCV to tune max_depth, min_samples_split, and criterion.\n",
        "5) Evaluate Performance:\n",
        "   - Use Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "Business Value:\n",
        "- The model can quickly identify at-risk patients.\n",
        "- Enables targeted treatment, reduces healthcare costs, and improves patient outcomes.\n"
      ],
      "metadata": {
        "id": "RlWXYE2bbxDe"
      }
    }
  ]
}