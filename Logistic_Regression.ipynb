{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Logistic Regression is a statistical model used for binary or multi-class classification problems.  \n",
        "It predicts the probability that an observation belongs to a certain class using the logistic (sigmoid) function.  \n",
        "Difference from Linear Regression:  \n",
        "- Linear Regression predicts continuous values, Logistic Regression predicts probabilities between 0 and 1.  \n",
        "- Logistic Regression uses the sigmoid function to map inputs to probability, whereas Linear Regression uses a straight-line equation.\n",
        "\n",
        "# 2. Explain the role of the Sigmoid function in Logistic Regression.\n",
        "The sigmoid function transforms any real-valued number into a value between 0 and 1, making it suitable for probability estimation.  \n",
        "Formula:  \n",
        "sigmoid(z) = 1 / (1 + e^(-z))  \n",
        "Here, z = b0 + b1X1 + b2X2 + ... + bnXn\n",
        "\n",
        "# 3. What is Regularization in Logistic Regression and why is it needed?\n",
        "Regularization is a technique to prevent overfitting by adding a penalty term to the loss function.  \n",
        "It discourages large coefficient values, improving generalization.  \n",
        "Types:  \n",
        "- L1 (Lasso): adds absolute value penalty |β|  \n",
        "- L2 (Ridge): adds squared value penalty β²\n",
        "\n",
        "# 4. What are some common evaluation metrics for classification models, and why are they important?\n",
        "- Accuracy: Proportion of correct predictions.  \n",
        "- Precision: True Positives / (True Positives + False Positives) — important when false positives are costly.  \n",
        "- Recall (Sensitivity): True Positives / (True Positives + False Negatives) — important when false negatives are costly.  \n",
        "- F1-Score: Harmonic mean of Precision and Recall — balances both metrics.  \n",
        "- ROC-AUC: Measures the model's ability to distinguish between classes.\n",
        "\n",
        "# 5. Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "\n",
        "# 6. Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
        "model_l2 = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "model_l2.fit(X_train, y_train)\n",
        "print(\"Coefficients:\", model_l2.coef_)\n",
        "print(\"Accuracy:\", model_l2.score(X_test, y_test))\n",
        "\n",
        "# 7. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "iris = load_iris()\n",
        "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "model_multi = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "model_multi.fit(X_train_i, y_train_i)\n",
        "y_pred_i = model_multi.predict(X_test_i)\n",
        "print(classification_report(y_test_i, y_pred_i))\n",
        "\n",
        "# 8. Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n",
        "\n",
        "# 9. Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without scaling\n",
        "model_no_scale = LogisticRegression(max_iter=1000)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "print(\"Accuracy without scaling:\", model_no_scale.score(X_test, y_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model_scale = LogisticRegression(max_iter=1000)\n",
        "model_scale.fit(X_train_scaled, y_train)\n",
        "print(\"Accuracy with scaling:\", model_scale.score(X_test_scaled, y_test))\n",
        "\n",
        "# 10. Imagine you are working at an e-commerce company...\n",
        "Steps:  \n",
        "1) Data Handling:  \n",
        "   - Handle missing values.  \n",
        "   - Encode categorical variables.  \n",
        "   - Remove irrelevant features.  \n",
        "2) Feature Scaling:  \n",
        "   - Standardize or normalize numerical features.  \n",
        "3) Balancing Classes:  \n",
        "   - Use SMOTE (Synthetic Minority Oversampling Technique) or class weighting in Logistic Regression.  \n",
        "4) Hyperparameter Tuning:  \n",
        "   - Use GridSearchCV to tune C, penalty, and solver parameters.  \n",
        "5) Model Evaluation:  \n",
        "   - Use Precision, Recall, F1-score, and ROC-AUC due to imbalanced data.  \n",
        "6) Business Context:  \n",
        "   - Prefer Recall to ensure most responders are captured, minimizing lost opportunities.\n"
      ],
      "metadata": {
        "id": "2-Qd9ol0VHx_"
      }
    }
  ]
}