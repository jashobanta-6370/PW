{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NepVuByFA-rs"
      },
      "outputs": [],
      "source": [
        "#1. What is Simple Linear Regression?\n",
        "Simple Linear Regression is a statistical method used to model the relationship between a dependent variable and one independent variable using a straight line.\n",
        "\n",
        "#2. What are the key assumptions of Simple Linear Regression?\n",
        "Key assumptions include linearity, independence of errors, homoscedasticity (constant variance), and normally distributed residuals.\n",
        "\n",
        "#3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "The coefficient m represents the slope, indicating the change in the dependent variable Y for a one-unit change in the independent variable X.\n",
        "\n",
        "#4. What does the intercept c represent in the equation Y = mX + c?\n",
        "The intercept c is the value of Y when X = 0; it represents the starting point of the regression line on the Y-axis.\n",
        "\n",
        "#5. How do we calculate the slope m in Simple Linear Regression?\n",
        "The slope m is calculated using the formula:\n",
        "m = Σ((Xi - X̄)(Yi - Ȳ)) / Σ((Xi - X̄)²),\n",
        "where X̄ and Ȳ are the means of X and Y, respectively.\n",
        "\n",
        "#6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "The least squares method minimizes the sum of squared differences between observed and predicted values, ensuring the best-fitting line.\n",
        "\n",
        "#7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "R² measures the proportion of variance in the dependent variable explained by the independent variable. Values closer to 1 indicate a better fit.\n",
        "\n",
        "#8. What is Multiple Linear Regression?\n",
        "Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables using a linear equation.\n",
        "\n",
        "#9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses two or more to predict the dependent variable.\n",
        "\n",
        "#10. What are the key assumptions of Multiple Linear Regression?\n",
        "Assumptions include linearity, no multicollinearity, homoscedasticity, independence, and normally distributed residuals.\n",
        "\n",
        "#11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Heteroscedasticity means the variance of errors varies across observations, which can lead to inefficient estimates and biased significance tests.\n",
        "\n",
        "#12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "You can improve it by removing highly correlated predictors, combining variables, or using techniques like Ridge Regression or Principal Component Analysis (PCA).\n",
        "\n",
        "#13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "Common techniques include one-hot encoding, label encoding, and creating dummy variables for categorical data.\n",
        "\n",
        "#14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "Interaction terms allow the effect of one variable to depend on the level of another, capturing combined effects not explained by individual variables.\n",
        "\n",
        "#15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "In simple regression, the intercept is the predicted Y when X is 0. In multiple regression, it’s the predicted Y when all independent variables are 0, which may not always be meaningful.\n",
        "\n",
        "#16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "The slope quantifies the relationship strength and direction between variables. A larger magnitude indicates a stronger effect of the predictor on the outcome.\n",
        "\n",
        "#17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "The intercept helps understand the baseline value of the dependent variable when all predictors are zero, anchoring the regression line.\n",
        "\n",
        "#18. What are the limitations of using R² as a sole measure of model performance?\n",
        "R² doesn’t indicate if the model is appropriate, can be artificially high with more variables, and doesn’t detect overfitting or multicollinearity.\n",
        "\n",
        "#19. How would you interpret a large standard error for a regression coefficient?\n",
        "A large standard error indicates high variability in the estimate, making the coefficient less reliable and possibly statistically insignificant.\n",
        "\n",
        "#20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Heteroscedasticity appears as changing spread in residual plots. It's important to address because it violates model assumptions and affects standard error estimates.\n",
        "\n",
        "#21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "It means the model may include irrelevant predictors. Adjusted R² accounts for the number of predictors and penalizes overfitting.\n",
        "\n",
        "#22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "Scaling ensures that all variables contribute equally to the model and helps algorithms converge faster, especially when interaction or regularization is used."
      ]
    }
  ]
}