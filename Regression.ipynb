{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Simple Linear Regression?\n",
        "Simple Linear Regression models the relationship between one independent variable X and one dependent variable Y.\n",
        "Equation: Y = m*X + c\n",
        "\n",
        "# 2. What are the key assumptions of Simple Linear Regression?\n",
        "1) Linearity: Y is a linear function of X.\n",
        "2) Independence: Observations are independent.\n",
        "3) Homoscedasticity: Constant variance of residuals.\n",
        "4) Normality: Residuals are approximately normally distributed.\n",
        "5) No perfect multicollinearity (not applicable with single predictor).\n",
        "\n",
        "# 3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "m is the slope. It represents the expected change in Y for a one-unit increase in X.\n",
        "\n",
        "# 4. What does the intercept c represent in the equation Y = mX + c?\n",
        "c is the intercept (bias). It is the predicted value of Y when X = 0.\n",
        "\n",
        "# 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "Formula:\n",
        "m = Σ((Xi - X̄)*(Yi - Ȳ)) / Σ((Xi - X̄)²)\n",
        "\n",
        "# 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "To find m and c that minimize the sum of squared residuals:\n",
        "SSE = Σ(Yi - (m*Xi + c))²\n",
        "\n",
        "# 7. How is the coefficient of determination (R²) interpreted?\n",
        "R² measures the proportion of variance in Y explained by the model.\n",
        "Formula:\n",
        "R² = 1 - (SS_res / SS_tot)\n",
        "\n",
        "# 8. What is Multiple Linear Regression?\n",
        "A regression with two or more independent variables predicting Y.\n",
        "Equation:\n",
        "Y = b0 + b1*X1 + b2*X2 + ... + bn*Xn\n",
        "\n",
        "# 9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple uses 1 predictor (X). Multiple uses 2 or more predictors (X1, X2, ...).\n",
        "\n",
        "# 10. What are the key assumptions of Multiple Linear Regression?\n",
        "1) Linearity in parameters\n",
        "2) Independence of errors\n",
        "3) Homoscedasticity\n",
        "4) Normality of residuals\n",
        "5) No multicollinearity\n",
        "\n",
        "# 11. What is heteroscedasticity, and how does it affect results?\n",
        "Heteroscedasticity = non-constant variance of residuals (e.g., funnel shape).\n",
        "It can bias standard errors, leading to incorrect significance tests.\n",
        "\n",
        "# 12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- Remove highly correlated variables\n",
        "- Use PCA (Principal Component Analysis)\n",
        "- Apply Ridge or Lasso regression\n",
        "\n",
        "# 13. What are some common techniques for transforming categorical variables?\n",
        "- One-hot encoding\n",
        "- Label encoding\n",
        "- Target encoding\n",
        "\n",
        "# 14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "Interaction terms capture the combined effect of two variables (e.g., X1*X2).\n",
        "\n",
        "# 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "Simple: Y when X = 0.\n",
        "Multiple: Y when all X variables = 0.\n",
        "\n",
        "# 16. What is the significance of the slope in regression analysis?\n",
        "It shows how much Y changes per 1-unit change in X.\n",
        "\n",
        "# 17. How does the intercept in a regression model provide context?\n",
        "It gives the baseline Y value when all predictors = 0.\n",
        "\n",
        "# 18. What are the limitations of using R² as a sole measure of model performance?\n",
        "- Does not imply causation\n",
        "- Increases with irrelevant predictors\n",
        "- Does not measure bias or model validity\n",
        "\n",
        "# 19. How would you interpret a large standard error for a regression coefficient?\n",
        "It means the coefficient is unstable and less reliable.\n",
        "\n",
        "# 20. How can heteroscedasticity be identified in residual plots?\n",
        "Look for patterns or funnel shapes in residual plots.\n",
        "\n",
        "# 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "It suggests irrelevant variables are inflating R² artificially.\n",
        "\n",
        "# 22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "To prevent predictors with large ranges from dominating and to help regularization methods.\n",
        "\n",
        "# 23. What is polynomial regression?\n",
        "A regression where the relationship is modeled as an nth-degree polynomial.\n",
        "Equation:\n",
        "Y = b0 + b1X + b2X² + ... + bnX^n\n",
        "\n",
        "# 24. How does polynomial regression differ from linear regression?\n",
        "Polynomial regression models curved relationships; linear regression models straight lines.\n",
        "\n",
        "# 25. When is polynomial regression used?\n",
        "When data shows a non-linear relationship between variables.\n",
        "\n",
        "# 26. What is the general equation for polynomial regression?\n",
        "Y = b0 + b1X + b2X² + ... + bnX^n\n",
        "\n",
        "# 27. Can polynomial regression be applied to multiple variables?\n",
        "Yes, by including polynomial terms for each predictor.\n",
        "\n",
        "# 28. What are the limitations of polynomial regression?\n",
        "- High degree polynomials may overfit\n",
        "- Sensitive to outliers\n",
        "- Poor at extrapolation\n",
        "\n",
        "# 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Cross-validation\n",
        "- Adjusted R²\n",
        "- AIC/BIC\n",
        "\n",
        "# 30. Why is visualization important in polynomial regression?\n",
        "To ensure the polynomial fits well without overfitting.\n",
        "\n",
        "# 31. How is polynomial regression implemented in Python?\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Example:\n",
        "# X = [[value1], [value2], ...]\n",
        "# y = [target1, target2, ...]\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "model = LinearRegression().fit(X_poly, y)\n"
      ],
      "metadata": {
        "id": "nI0NP-2kRlIP"
      }
    }
  ]
}