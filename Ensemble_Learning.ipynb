{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "1wlYA8PJMBPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Can we use Bagging for regression problems?\n",
        "Yes. Bagging can be applied to regression tasks by training multiple regressors on bootstrap samples and averaging their predictions.\n",
        "\n",
        "# 2. What is the difference between multiple model training and single model training?\n",
        "- Single model training: One model is trained on the entire dataset.\n",
        "- Multiple model training: Multiple models are trained and combined to improve performance, as in ensemble methods.\n",
        "\n",
        "# 3. Explain the concept of feature randomness in Random Forest.\n",
        "In Random Forest, feature randomness means that at each split, only a random subset of features is considered, which increases diversity among trees and reduces correlation.\n",
        "\n",
        "# 4. What is OOB (Out-of-Bag) Score?\n",
        "OOB score is the evaluation metric obtained from data samples not included in the bootstrap sample for a given tree, serving as an internal cross-validation score.\n",
        "\n",
        "# 5. How can you measure the importance of features in a Random Forest model?\n",
        "By calculating how much each feature decreases impurity (Gini or entropy) across all trees or using permutation importance.\n",
        "\n",
        "# 6. Explain the working principle of a Bagging Classifier.\n",
        "Bagging trains multiple base classifiers on bootstrap samples and aggregates their predictions (majority voting) to make the final decision.\n",
        "\n",
        "# 7. How do you evaluate a Bagging Classifierâ€™s performance?\n",
        "By using metrics like accuracy, precision, recall, F1-score, ROC-AUC, or cross-validation scores.\n",
        "\n",
        "# 8. How does a Bagging Regressor work?\n",
        "It trains multiple regressors on bootstrap samples and averages their predictions to produce the final output.\n",
        "\n",
        "# 9. What is the main advantage of ensemble techniques?\n",
        "They improve prediction accuracy, reduce variance, and increase robustness compared to single models.\n",
        "\n",
        "# 10. What is the main challenge of ensemble methods?\n",
        "They can be computationally expensive and harder to interpret compared to single models.\n",
        "\n",
        "# 11. Explain the key idea behind ensemble techniques.\n",
        "The key idea is to combine multiple weak learners to create a strong learner by leveraging diversity among models.\n",
        "\n",
        "# 12. What is a Random Forest Classifier?\n",
        "A Random Forest Classifier is an ensemble of decision trees where each tree is trained on a bootstrap sample and uses random subsets of features for splitting.\n",
        "\n",
        "# 13. What are the main types of ensemble techniques?\n",
        "- Bagging\n",
        "- Boosting\n",
        "- Stacking\n",
        "\n",
        "# 14. What is ensemble learning in machine learning?\n",
        "Ensemble learning is the process of combining predictions from multiple models to improve performance.\n",
        "\n",
        "# 15. When should we avoid using ensemble methods?\n",
        "When interpretability is critical or when computational resources are very limited.\n",
        "\n",
        "# 16. How does Bagging help in reducing overfitting?\n",
        "By averaging predictions from multiple models trained on different samples, Bagging reduces variance and overfitting.\n",
        "\n",
        "# 17. Why is Random Forest better than a single Decision Tree?\n",
        "It reduces overfitting, improves generalization, and increases accuracy by combining multiple decision trees.\n",
        "\n",
        "# 18. What is the role of bootstrap sampling in Bagging?\n",
        "Bootstrap sampling creates multiple datasets from the original by random sampling with replacement, ensuring diversity among models.\n",
        "\n",
        "# 19. What are some real-world applications of ensemble techniques?\n",
        "- Fraud detection\n",
        "- Medical diagnosis\n",
        "- Credit scoring\n",
        "- Customer churn prediction\n",
        "\n",
        "# 20. What is the difference between Bagging and Boosting?\n",
        "- Bagging: Models are trained independently on different samples.\n",
        "- Boosting: Models are trained sequentially, with each model focusing on correcting errors of the previous one.\n"
      ],
      "metadata": {
        "id": "ulCSKRWNMEWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", bag_clf.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHi844IyMNAg",
        "outputId": "fc394c38-2598-46c4-fad2-66254a7077fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Train a Bagging Regressor using Decision Trees and evaluate using MSE.\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
        "    housing.data, housing.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "bag_reg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train_h, y_train_h)\n",
        "y_pred_h = bag_reg.predict(X_test_h)\n",
        "print(\"MSE:\", mean_squared_error(y_test_h, y_pred_h))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl_Wk-q_cKVb",
        "outputId": "d574284f-4ed2-4c8b-ef88-8a893e018a9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.2572988359842641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train_c, y_train_c)\n",
        "print(\"Feature Importances:\", rf_clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GxFnZneMRJh",
        "outputId": "eef5468d-9d15-4c03-b858-0401d8d9f97c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.04870337 0.01359088 0.05326975 0.04755501 0.00728533 0.01394433\n",
            " 0.06800084 0.10620999 0.00377029 0.00388577 0.02013892 0.00472399\n",
            " 0.01130301 0.02240696 0.00427091 0.00525322 0.00938583 0.00351326\n",
            " 0.00401842 0.00532146 0.07798688 0.02174901 0.06711483 0.15389236\n",
            " 0.01064421 0.02026604 0.0318016  0.14466327 0.01012018 0.00521012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "rf_reg.fit(X_train_h, y_train_h)\n",
        "dt_reg.fit(X_train_h, y_train_h)\n",
        "\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test_h, rf_reg.predict(X_test_h)))\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test_h, dt_reg.predict(X_test_h)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCj3Bt0-MVZw",
        "outputId": "ddfc2075-ce1a-49e3-e535-558678eb40ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest MSE: 0.2553684927247781\n",
            "Decision Tree MSE: 0.495235205629094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_oob.fit(X_train_c, y_train_c)\n",
        "print(\"OOB Score:\", rf_oob.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph8Elf-KMbE5",
        "outputId": "1ae29a3e-4e3d-4d11-e4d4-d71b8345e837"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9560439560439561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "from sklearn.svm import SVC\n",
        "bag_svm = BaggingClassifier(SVC(), n_estimators=10, random_state=42)\n",
        "bag_svm.fit(X_train_c, y_train_c)\n",
        "print(\"Accuracy:\", bag_svm.score(X_test_c, y_test_c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbjpyxlOMcvQ",
        "outputId": "fb923e94-75d1-47e6-9474-a4a8f5230e60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
        "for n in [10, 50, 100]:\n",
        "    rf_var = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf_var.fit(X_train_c, y_train_c)\n",
        "    print(f\"n_estimators={n}, Accuracy={rf_var.score(X_test_c, y_test_c)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBb8rVbeMeiZ",
        "outputId": "6b4576df-2bfd-4755-c2a9-36b3257a7af8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_estimators=10, Accuracy=0.956140350877193\n",
            "n_estimators=50, Accuracy=0.9649122807017544\n",
            "n_estimators=100, Accuracy=0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "bag_lr = BaggingClassifier(LogisticRegression(max_iter=1000), n_estimators=10, random_state=42)\n",
        "bag_lr.fit(X_train_c, y_train_c)\n",
        "y_prob_lr = bag_lr.predict_proba(X_test_c)[:, 1]\n",
        "print(\"AUC Score:\", roc_auc_score(y_test_c, y_prob_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81kJsZcNMgtp",
        "outputId": "741d597c-8f08-42b6-b202-c35d6f86e03b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9980347199475925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Train a Random Forest Regressor and analyze feature importance scores.\n",
        "rf_reg_imp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg_imp.fit(X_train_h, y_train_h)\n",
        "print(\"Feature Importances:\", rf_reg_imp.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAq0Qyjze_n8",
        "outputId": "5123ad29-e77c-4a69-a308-cd3d23d4010a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.52487148 0.05459322 0.04427185 0.02960631 0.03064978 0.13844281\n",
            " 0.08893574 0.08862881]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "bag_clf_comp = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag_clf_comp.fit(X_train_c, y_train_c)\n",
        "rf_clf_comp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf_comp.fit(X_train_c, y_train_c)\n",
        "print(\"Bagging Accuracy:\", bag_clf_comp.score(X_test_c, y_test_c))\n",
        "print(\"Random Forest Accuracy:\", rf_clf_comp.score(X_test_c, y_test_c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6QkcT8FfANF",
        "outputId": "2fdd9c3e-d20f-4287-fad6-e8b5122b67a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 0.956140350877193\n",
            "Random Forest Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    }
  ]
}